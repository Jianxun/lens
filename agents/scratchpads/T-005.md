# T-005 — Ingestion pipeline

## Task summary
- DoD: JSONL ingest into Postgres with deterministic idx, truncation warnings, user/assistant filter, raw retention, ingest_runs logging; CLI entrypoint; ingest_runs shows succeeded with stats; spot-check messages with idx and timestamps.
- Verify: run ingest on sample JSONL; ingest_runs row status=succeeded with stats; messages persisted with idx and timestamps.

## Read
- agents/context/tasks.md
- agents/context/handoff.md
- agents/adr/ADR-0002-ingestion-pipeline.md
- migrations/0001_init.sql
- README.md
- data/chatgpt_dump/2025-12-23/conversations.jsonl (structure probe via Python)

## Plan
- Build ingestion module under backend/ingest with deterministic traversal, truncation, role filter, stats, ingest_runs handling.
- Create CLI in scripts/ to run ingestion using env-based DB config.
- Update README with ingest usage and verify steps on sample JSONL.

## Progress log
- Initialized branch feature/T-005-ingestion; reviewed ADR-0002, schema, sample JSONL to confirm fields (mapping tree, mixed content structures).
- Planning traversal: DFS over mapping roots/children sorted by (timestamp, node id); increment idx on kept messages; parent_id = kept parent message if any.
- Content handling: join content.parts strings when present; fall back to user_instructions/text; store full content JSON in content_parts; truncate content_text >32000 chars (record truncations); also guard turn_summary to 4000 to avoid constraint violations.
- Ingest runs: insert running row, process file, update status/stats/error on completion/failure.
- Implemented backend ingestion pipeline with null-byte scrubbing, deterministic DFS traversal, role filtering, length truncation, ingest_runs logging, and conversation/message upserts.
- Added CLI `scripts/ingest_jsonl.py` with repo-root PYTHONPATH shim; dependency pin `backend/ingest/requirements.txt`; README updated with ingest usage/verify steps; codebase_map updated for new dirs.
- Updated pipeline to skip empty/blank content_text (counts tracked). Dropped tables, reapplied migration, reran ingest on sample JSONL via Postgres (port 5433) after change. Stats: conversations_written=2059, conversations_skipped=0, messages_written=27538, messages_role_skipped=15020, messages_content_empty_skipped=12827, messages_truncated=25, turn_summary_truncated=0, lines_processed=2059. Cleared prior ingest_runs by recreate.

## Patch summary
- backend/__init__.py (new backend package)
- backend/ingest/pipeline.py (ingest pipeline, null scrubbing, traversal, stats, ingest_runs handling, skip empty content_text)
- backend/ingest/__init__.py, requirements.txt (exports + deps)
- scripts/ingest_jsonl.py (CLI entrypoint with PYTHONPATH setup)
- README.md (ingest usage and verify commands)
- agents/context/codebase_map.md, agents/context/tasks.md, agents/context/handoff.md (status + map + handoff updates)

## Verification
- POSTGRES_PORT=5433 .venv/bin/python scripts/ingest_jsonl.py data/chatgpt_dump/2025-12-23/conversations.jsonl (after table drop + migration reapply) → succeeded with stats: {'conversations_written': 2059, 'conversations_skipped': 0, 'messages_written': 27538, 'messages_role_skipped': 15020, 'messages_content_empty_skipped': 12827, 'messages_truncated': 25, 'turn_summary_truncated': 0, 'lines_processed': 2059}
- docker compose exec -T db psql -U lens -d lens -c "\\dt" → expected tables present.
- docker compose exec -T db psql -U lens -d lens -c "select status, stats from ingest_runs order by started_at desc limit 1" → latest status=succeeded with stats above.
- docker compose exec -T db psql -U lens -d lens -c "select id, conversation_id, idx_in_conv, role, create_time from messages order by create_time asc limit 5" → sequential idx_in_conv/timestamps.

## Blockers / Questions
- None yet.

## Next steps
- None; task DoD met. If dataset changes, rerun ingest CLI (defaults to POSTGRES_* env vars).

