# T-010 — Embedding creation pipeline

## Task summary
- DoD
  - Implement embedding job to generate turn-level embeddings (user anchor, assistant linked) using fixed `text-embedding-3-large`.
  - Respect truncation limits; store `content_used`, `used_turn_summary`, `content_hash`; upsert into `message_embeddings` with unique (user_message_id, provider, model).
  - CLI/runner to process new messages incrementally; skip already-hashed content.
- Verify
  - Run on sample data completes; embeddings table populated with expected rows and vectors; duplicates avoided via unique constraint.
  - Spot-check summary preference: when turn_summary exists, it is used.

## Read
- agents/context/lessons.md
- agents/context/contract.md
- agents/context/tasks.md
- agents/context/handoff.md
- agents/scratchpads/T-010.md (prior content)

## Plan
1) Survey existing DB schema, ingest outputs, and any embedding-related utilities to align fields/hashes/truncation.  
2) Design embedding pipeline: text selection (summary vs assistant), truncation guard, hashing, incremental selection (missing or hash mismatch), batching and retries around embedding API.  
3) Implement backend modules under `backend/embeddings/` plus CLI in `scripts/` to run against DB; document usage in README.  
4) Run sample embedding job to populate `message_embeddings`, validate summary preference and dedup, then update context files.

## Progress log
- 2025-12-25: Read lessons/contract/tasks/handoff; created branch `feature/T-010-embedding-pipeline`; set T-010 to In Progress; rebuilt scratchpad structure.
- 2025-12-25: Inspected schema (messages, message_embeddings), ADR-0003, ingest pipeline behavior; decided to anchor selection on assistant with parent user, skipping empty content and reusing turn_summary when present.
- 2025-12-25: Implemented embedding pipeline (`backend/embeddings/pipeline.py`) with student portal client (provider supermind), SHA256 content_hash, truncation guard, batching/retries, upsert with hash skip; added CLI `scripts/embed_messages.py`, requirements, README usage. Smoke-tested student portal embeddings manually (3072-dim returned).
- 2025-12-25: Installed embedding deps into `.venv`; ran sample job (`scripts/embed_messages.py --batch-size 8 --limit 20`) with env from `.env` → embedded=8, skipped_existing_hash=12, batches=1.
- 2025-12-25: Added per-batch progress prints in embedding pipeline showing batch number, embedded, skipped_hash, total embedded, processed.
- 2025-12-25: Added `--force` mode to recompute embeddings even when hashes match; documented in README.
- 2025-12-25: Added offset-based paging to avoid reprocessing; ran forced job on port 5432 with limit 100 → `message_embeddings` rows=100 for provider/model `supermind`/`text-embedding-3-large`.

## Patch summary
- `backend/embeddings/pipeline.py`: Embedding job with candidate selection, content build, hash skip, supermind API client, and upsert into `message_embeddings`.
- `backend/embeddings/__init__.py`: Package init.
- `backend/embeddings/requirements.txt`: httpx + psycopg deps for embedding job.
- `scripts/embed_messages.py`: CLI runner with batching/limit/config flags.
- `README.md`: Embedding run instructions (deps, env, sample command).

## Verification
- Student portal embedding smoke test (model text-embedding-3-large) returned HTTP 200, embedding length 3072, sample values.
- Sample DB embedding job: `.venv/bin/python scripts/embed_messages.py --batch-size 8 --limit 20` (with env from `.env`) → `{'embedded': 8, 'skipped_existing_hash': 12, 'batches': 1}`.
- Port-5432 forced job with paging: `.venv/bin/python scripts/embed_messages.py --batch-size 8 --limit 100 --force` → `message_embeddings` rows=100.

## Blockers / Questions
- None yet.

## Next steps
1) If dataset changes, rerun full embedding job (no `--limit`) after ingest with `SUPER_MIND_API_KEY` set.
2) Consider vector index if switching to <=2000-dim model; otherwise expect seq scans.
3) Proceed to retrieval/chat tasks using populated `message_embeddings`.

